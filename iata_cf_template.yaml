Parameters:

# In case prefered the resource names to be entered upon creation in CF
  ZipBucketName:
    Type: String

  ParquetBucketName:
    Type: String

  ZipFunctionName:
    Type: String

  ParquetFunctionName:
    Type: String

  RoleName:
    Type: String

  QueueName:
    Type: String

  GlueTrigger:
    Type: String

#  GlueCrawler:
#    Type: String

Resources:


  SQSdownloadFile:
    Type: 'AWS::SQS::Queue'
    DeletionPolicy: Delete
    Properties:
      QueueName: !Ref QueueName
    DependsOn:
      - DownloadMessageURL

  DownloadMessageURL:
    Type: 'AWS::Lambda::Function'
    DeletionPolicy: Delete
    DependsOn:
      - ZipFile
      - UnzipConvertParquet
    Properties:
      FunctionName: !Ref ZipFunctionName
      Role: !GetAtt IAMLambdaRole.Arn
      Runtime: python3.9
      Handler: index.my_handler
      Code:
        ZipFile: |
          import json
          import boto3
          import urllib3
          import os
          import requests
          def lambda_handler(event, context):
            # get the url
            url = event['Records'][0]['body'].split('"')[-2]
            bucket = 'iata-sales-record-archive'        
            zipfile = url.split('/')[-1]
            key = zipfile
            # try to stream the URL directly into S3
            s3=boto3.client('s3')
            http=urllib3.PoolManager()
            # NOTE: I try to stream the URL directly to S3, however it gives an empty file
            #s3.upload_fileobj(http.request('GET', url, preload_content=False), bucket, key)
            # As an alternative, write to local filesystem (/tmp) using request
            req = requests.get(url)
            file = '/tmp/req' + zipfile
            with open(file,'wb') as output_file:
                output_file.write(req.content)
                print('Downloading Completed')
            s3.put_object(Bucket=bucket,Key=file)
            return {
                'statusCode': 200,
                'body': json.dumps('zip file downloaded')
            }

    


  
  IAMLambdaRole:
    Type: AWS::IAM::Role
    DeletionPolicy: Delete
    Properties:
      RoleName: !Ref RoleName
      AssumeRolePolicyDocument:
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole

      
  MyInvokeZipPermission:
    Type: AWS::Lambda::Permission
    DeletionPolicy: Delete
    DependsOn: DownloadMessageURL
    Properties:
      FunctionName:
        Fn::GetAtt:
          - DownloadMessageURL
          - Arn     
      Action: lambda:InvokeFunction
      Principal: s3.amazonaws.com
      SourceArn:
        Fn::Sub: arn:aws:s3:::${ZipBucketName} 
      

  ZipFile:
    Type: 'AWS::S3::Bucket'
    DeletionPolicy: Delete
    DependsOn: UnzipConvertParquet
    Properties:
      BucketName: !Ref ZipBucketName
      #NotificationConfiguration:
      #  LambdaConfigurations:
      #    - Event: s3:ObjectCreated:Put
      #      Function: !GetAtt DownloadMessageURL.Arn
   


  UnzipConvertParquet:
    Type: 'AWS::Lambda::Function'
    DeletionPolicy: Delete
    DependsOn:
        -  ParquetFile
    Properties:
      FunctionName: !Ref ParquetFunctionName
      Role: !GetAtt IAMLambdaRole.Arn
      Runtime: python3.9
      Handler: index.my_handler
      Code:
        ZipFile: |
          import json
          import boto3
          import os
          import pandas as pd
          import awswrangler as wr
          from zipfile import ZipFile
          def lambda_handler(event, context):
            # import .zip data from S3 bucket
            inbucket = 'iata-sales-record-archive'
            outbucket = 'iata-parquet-data'
            s3 = boto3.client("s3")
            for obj in s3.list_objects_v2(Bucket=inbucket)['Contents']:
                # look for .zip files in the root directory
                file = obj['Key'].split('/')[0]
                print(file)
                if file[-4:] == '.zip':
                    # a zip file is recognised
                    # extract the file and write to parquet
                    infile = obj['Key']
                    outfile = '/tmp/temp.zip'
                    s3.download_file(inbucket, infile, outfile)
                    with ZipFile(outfile, 'r') as zip:
                        zip.extractall('/tmp/')
                    files = os.listdir('/tmp/')
                    # rename .csv file if it contains spaces
                    for f in files:
                    if f[-4:] == '.csv':
                        if ' ' in f:
                            new_name = str.replace(f, ' ','_')
                            print(f)
                            print(new_name)
                            os.rename('/tmp/' + f, '/tmp/' + new_name)
                    print('rename finished')
                    # write the file to s3 
                    s3 = boto3.resource('s3')
                    s3.Bucket(outbucket).upload_file('/tmp/' + new_name, new_name)
            ##############################
            # convert the csv file to parquet
            files = os.listdir('/tmp/')
            for f in files:
            # check for csv files
            if f[-4:] == '.csv':
                data = pd.read_csv('/tmp/' + f)
                #df.to_parquet('output.parquet')
                output_file = 's3://iata-parquet-data/' + file[0:-4] + '.parquet'  
                wr.s3.to_parquet(
                df=data,
                path=output_file,
                dataset=True,
                partition_cols=['Country']
                )
            return {
                'statusCode': 200,
                'body': json.dumps('data has been converted to parquet')
            }

    
  MyInvokeParquetPermission:
    Type: AWS::Lambda::Permission
    DeletionPolicy: Delete
    DependsOn: UnzipConvertParquet
    Properties:
      FunctionName:
        Fn::GetAtt:
          - UnzipConvertParquet
          - Arn        
      Action: lambda:InvokeFunction
      Principal: s3.amazonaws.com
      SourceArn:
        Fn::Sub: arn:aws:s3:::${ParquetBucketName} 
  



  ParquetFile:
    Type: 'AWS::S3::Bucket'
    DeletionPolicy: Delete
    DependsOn: SQSTriggerGlue
    Properties:
      BucketName: !Ref ParquetBucketName
      #NotificationConfiguration:
      #  LambdaConfigurations:
      #    - Event: s3:ObjectCreated:Put
      #      Function: !GetAtt UnzipConvertParquet.Arn
      
      
 


  SQSTriggerGlue:
    Type: 'AWS::SQS::Queue'
    DeletionPolicy: Delete
#    DependsOn: GlueCreateDB
    Properties:
      QueueName: !Ref GlueTrigger
      
#  GlueCreateDB:
#    Type: 'AWS::Glue::Crawler'    
#    DeletionPolicy: Delete
#    Properties:
#      Name: !Ref GlueCrawler
    
     
      